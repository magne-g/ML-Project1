import hashlib
import os
import warnings
from time import time
from matplotlib import pyplot as py
from datetime import date
from pylatex import Document, Table, Tabular, LongTable, MultiColumn, Subsection, Subsubsection, FlushLeft, Figure, \
    SubFigure
from sklearn import tree
import numpy as np
import pandas as pd
import seaborn as sns
import math
import matplotlib.colors
from scipy.stats import randint as sp_randint
from scipy.stats import rv_continuous
from scipy.stats import rv_discrete
import sklearn.utils.testing as test
from IPython.core.interactiveshell import InteractiveShell
from matplotlib import pyplot as plt
from sklearn import metrics as metrics, preprocessing
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor, RandomForestRegressor, \
    ExtraTreesRegressor
from sklearn.externals.six import StringIO
from sklearn.kernel_ridge import KernelRidge
from sklearn.linear_model import BayesianRidge, Lasso
from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, learning_curve, train_test_split, \
    validation_curve, cross_val_predict, RandomizedSearchCV
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA, SparsePCA

#Preprocess this dataset in the simplest possible way so that the re-
#sponse can be perfectly predicted using just one single test of the form
#z < c, where c is a constant and z is a new feature generated by your preprocessing.

d = {'x': [3.874093, 38.065621, 151.304958, 252.845972, 1.537322, 388.705739, 1.151664, 1.557184, 12.459530, 1.073156,
          4.348563, 14.912576, 153.259792],
    'y': [1.993441, 3.158144, 5.955080, 0.074435, 0.098415, 20.633984, 0.007804, 0.186299, 3.792753, 0.002964, 2.075805,
          31.046062, 6.194166]}

y = {'r': [0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0]}

d2 = {'x': [3.874093, 38.065621, 151.304958, 252.845972, 1.537322, 388.705739, 1.151664, 1.557184, 12.459530, 1.073156,
          4.348563, 14.912576, 153.259792]}

X = pd.DataFrame.from_dict(d)

P = pd.DataFrame.from_dict(d2)

y = pd.DataFrame.from_dict(y)

from sklearn.tree import _tree


def tree_to_code(tree, feature_names):
        tree_ = tree.tree_
        feature_name = [
            feature_names[i] if i != _tree.TREE_UNDEFINED else "undefined!"
            for i in tree_.feature
        ]
        print("def tree({}):".format(", ".join(feature_names)))

        def recurse(node, depth):
            indent = "  " * depth
            if tree_.feature[node] != _tree.TREE_UNDEFINED:
                name = feature_name[node]
                threshold = tree_.threshold[node]
                print("{}if {} <= {}:".format(indent, name, threshold))
                recurse(tree_.children_left[node], depth + 1)
                print("{}else:  # if {} > {}".format(indent, name, threshold))
                recurse(tree_.children_right[node], depth + 1)
            else:
                print("{}return {}".format(indent, tree_.value[node]))

        recurse(0, 1)


print(X['x'].cov(X['y']))

#print("Minimum Values:")
#print(X[['x', 'y']].min(axis=0))
#print("Maximum Values:")
#print(X[['x', 'y']].max(axis=0))

pMin = min(X[['x', 'y']].min(axis=0))
pMax = max(X[['x', 'y']].max(axis=0))

ratio = np.round(pMax/pMin)

#print(X)

#sns.distplot(X['x'])

#plt.show()

#sns.distplot(X['y'])

#plt.show()

X2 = X

sns.kdeplot(X['x'],X['y'], shade=True, cbar=True)





plt.show()

print(ratio)
X[['x', 'y']] = preprocessing.normalize(X[['x', 'y']])
#X[['x', 'y']] = preprocessing.scale(X[['x', 'y']])


#X[['x', 'y']] = preprocessing.PowerTransformer(method='box-cox', copy=True, standardize=True).fit_transform(X[['x', 'y']])

#X[['x', 'y']] = preprocessing.StandardScaler().fit_transform(X[['x', 'y']])



sns.kdeplot(X['x'],X['y'], shade=True, cbar=True)
print(X)
print(X.skew())
plt.show()

X['r'] = y



pca = PCA(1)

#Z = pca.fit_transform(X, y)
Z2 = pca.fit_transform(X)
#Z = scaler.fit_transform(Z)
print("Principal Component")
print(Z2)







print(pca.components_)
print(pca.explained_variance_)
print(pca.singular_values_)


clf = DecisionTreeClassifier()
clf.fit(Z2, y)

p = clf.predict(Z2)
print(p)

print("Accuracy Score:")
print(metrics.balanced_accuracy_score(y, p))

n_dic = [[],[]]
for e in Z2:
    n_dic.append(e[0])



A = pd.read_csv('../calc.csv')

print(A.to_latex(longtable=True))


print(A)

tree_to_code(clf, feature_names=['x','y', 'r'])


